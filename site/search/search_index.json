{"config":{"lang":["fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GestureMouseApp","text":"<p>Contr\u00f4lez votre ordinateur par gestes - Une solution intuitive pour une interaction naturelle</p> <p>Commencer Voir la d\u00e9mo</p>"},{"location":"#fonctionnalites-principales","title":"Fonctionnalit\u00e9s principales","text":"<ul> <li>\ud83d\uddb1\ufe0f Contr\u00f4le de la souris par reconnaissance gestuelle</li> <li>\ud83d\udd0a Ajustement du volume par gestes</li> <li>\ud83d\udca1 Contr\u00f4le de la luminosit\u00e9 de l'\u00e9cran</li> <li>\u2753 Module FAQ int\u00e9gr\u00e9 avec chatbot intelligent</li> <li>\u2699\ufe0f Interface utilisateur personnalisable</li> </ul>   -   :fontawesome-solid-hand-pointer:{ .lg .middle } __Contr\u00f4le gestuel__      ---      - D\u00e9placement du curseur     - Clics gauche/droit     - D\u00e9filement     - Zoom  -   :material-cog:{ .lg .middle } __Personnalisation__      ---      - Sensibilit\u00e9 ajustable     - Gestes personnalisables     - Th\u00e8mes d'interface     - Profils utilisateur  -   :material-chat-processing:{ .lg .middle } __Assistance__      ---      - FAQ int\u00e9gr\u00e9e     - D\u00e9tection automatique des probl\u00e8mes     - Guide contextuel     - Journalisation d\u00e9taill\u00e9e"},{"location":"#captures-decran","title":"Captures d'\u00e9cran","text":"Interface principale Param\u00e8tres FAQ"},{"location":"guide/installation/","title":"Installation","text":""},{"location":"guide/installation/#prerequis","title":"Pr\u00e9requis","text":"<ul> <li>Windows 10/11 (64-bit)</li> <li>Webcam compatible</li> <li>Microsoft Visual C++ Redistributable</li> </ul>"},{"location":"guide/installation/#methodes-dinstallation","title":"M\u00e9thodes d'installation","text":""},{"location":"guide/installation/#installation-a-partir-du-code-source","title":"Installation \u00e0 partir du code source","text":"<ol> <li> <p>Cloner le d\u00e9p\u00f4t :    ```bash    git clone https://github.com/Marc1T/gestureControl.git    cd GestureMouseApp</p> </li> <li> <p>Cr\u00e9er un environnement virtuel :    <pre><code>python -m venv .venv\n.\\.venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Installer les d\u00e9pendances :    <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Lancer l'application :    <pre><code>python main.py\n</code></pre></p> </li> </ol>"},{"location":"guide/installation/#installation-via-lexecutable","title":"Installation via l'ex\u00e9cutable","text":"<ol> <li>T\u00e9l\u00e9charger la derni\u00e8re version depuis GitHub Releases</li> <li>Ex\u00e9cuter <code>GestureMouseApp-Setup.exe</code></li> <li>Suivre les instructions d'installation</li> </ol>"},{"location":"guide/installation/#installation-avec-pip","title":"Installation avec pip","text":"<pre><code>pip install gesturemouseapp\ngesturemouseapp\n</code></pre>"},{"location":"guide/installation/#verification-de-linstallation","title":"V\u00e9rification de l'installation","text":"<p>Apr\u00e8s installation, lancez l'application et v\u00e9rifiez :</p> <ol> <li>La webcam est d\u00e9tect\u00e9e</li> <li>Les gestes sont reconnus dans l'onglet Vid\u00e9o</li> <li>Le chatbot r\u00e9pond aux questions dans l'onglet FAQ</li> </ol>"},{"location":"implementation/architecture/","title":"Architecture du Syst\u00e8me","text":"<p>GestureMouseApp suit une architecture modulaire avec une s\u00e9paration claire des responsabilit\u00e9s :</p> <pre><code>graph TD\n    A[main.py] --&gt; B[Interface]\n    A --&gt; C[Core]\n    A --&gt; D[Utils]\n    B --&gt; E[MainWindow]\n    B --&gt; F[VideoThread]\n    C --&gt; G[GestureDetection]\n    C --&gt; H[HandRecognition]\n    C --&gt; I[GestureController]\n    D --&gt; J[ConfigManager]\n    D --&gt; K[Logger]</code></pre>"},{"location":"implementation/architecture/#composants-cles","title":"Composants Cl\u00e9s","text":"<ol> <li>Couche Interface (PyQt5) :</li> <li>Gestion des fen\u00eatres et dialogues</li> <li>Thread vid\u00e9o pour le traitement en temps r\u00e9el</li> <li> <p>Communication via les signaux/slots</p> </li> <li> <p>Couche Core (Logique M\u00e9tier) :</p> </li> <li>D\u00e9tection des gestes avec MediaPipe</li> <li>Reconnaissance des gestes sp\u00e9cifiques</li> <li> <p>Ex\u00e9cution des actions syst\u00e8me</p> </li> <li> <p>Couche Utils (Services) :</p> </li> <li>Gestion de configuration (INI/JSON)</li> <li>Journalisation centralis\u00e9e</li> <li>Fonctions utilitaires</li> </ol>"},{"location":"implementation/architecture/#flux-de-donnees","title":"Flux de Donn\u00e9es","text":"<pre><code>sequenceDiagram\n    Utilisateur-&gt;&gt;Interface: Interagit avec l'UI\n    Interface-&gt;&gt;VideoThread: D\u00e9marre le flux vid\u00e9o\n    VideoThread-&gt;&gt;GestureDetection: Envoie les frames\n    GestureDetection-&gt;&gt;HandRecognition: Analyse les landmarks\n    HandRecognition-&gt;&gt;GestureController: Identifie le geste\n    GestureController-&gt;&gt;Syst\u00e8me: Ex\u00e9cute l'action\n    GestureController-&gt;&gt;Interface: Retourne le feedback</code></pre>"},{"location":"implementation/architecture/#principes-de-conception","title":"Principes de Conception","text":"<ul> <li>D\u00e9couplage : Les modules communiquent via des interfaces d\u00e9finies</li> <li>Configurable : Tous les param\u00e8tres sont externalis\u00e9s</li> <li>Extensible : Ajout facile de nouveaux gestes ou actions</li> <li>Robuste : Gestion d'erreurs \u00e0 tous les niveaux</li> </ul>"},{"location":"implementation/config-management/","title":"Gestion de Configuration","text":""},{"location":"implementation/config-management/#systeme-hierarchique","title":"Syst\u00e8me Hi\u00e9rarchique","text":"<pre><code>graph LR\n    A[settings.ini] --&gt; B[Param\u00e8tres g\u00e9n\u00e9raux]\n    A --&gt; C[Param\u00e8tres UI]\n    A --&gt; D[Param\u00e8tres d\u00e9tection]\n    E[gestures.json] --&gt; F[Mapping gestes-actions]\n    G[faq.json] --&gt; H[Questions/R\u00e9ponses]\n</code></pre>"},{"location":"implementation/config-management/#chargement-et-validation","title":"Chargement et Validation","text":"<p>Le ConfigManager valide tous les param\u00e8tres :</p> <pre><code>def validate_param(self, key: str, value: Any) -&gt; Any:\n    rules = {\n        \"sensitivity\": (int, 0, 100),\n        \"threshold\": (float, 0.1, 1.0)\n    }\n\n    if key in rules:\n        param_type, min_val, max_val = rules[key]\n        val = param_type(value)\n        return clamp(val, min_val, max_val)\n</code></pre>"},{"location":"implementation/config-management/#mecanisme-de-fallback","title":"M\u00e9canisme de Fallback","text":"<ol> <li>Tente de lire la valeur configur\u00e9e</li> <li>Si absente ou invalide, utilise la valeur par d\u00e9faut</li> <li>Enregistre un avertissement dans les logs</li> </ol>"},{"location":"implementation/faq-system/","title":"Syst\u00e8me FAQ Intelligent","text":""},{"location":"implementation/faq-system/#architecture-nlp","title":"Architecture NLP","text":"<pre><code>graph TD\n    A[Question Utilisateur] --&gt; B[Pr\u00e9traitement]\n    B --&gt; C[Vectorisation TF-IDF]\n    C --&gt; D[Similarit\u00e9 Cosinus]\n    D --&gt; E[S\u00e9lection R\u00e9ponse]</code></pre>"},{"location":"implementation/faq-system/#pretraitement-du-texte","title":"Pr\u00e9traitement du Texte","text":"<pre><code>def _preprocess_text(self, text: str) -&gt; str:\n    # 1. Nettoyage\n    text = re.sub(r'[^\\w\\s]', '', text.lower())\n\n    # 2. Tokenisation\n    tokens = word_tokenize(text, language=\"french\")\n\n    # 3. Filtrage et stemming\n    tokens = [self.stemmer.stem(t) for t in tokens \n              if t not in self.stop_words and len(t) &gt; 2]\n\n    return \" \".join(tokens)\n</code></pre>"},{"location":"implementation/faq-system/#processus-de-matching","title":"Processus de Matching","text":"<ol> <li>Vectorisation des questions FAQ et de la requ\u00eate</li> <li>Calcul de similarit\u00e9 cosinus</li> <li>S\u00e9lection de la r\u00e9ponse avec le score le plus \u00e9lev\u00e9</li> <li>Application d'un seuil de confiance (0.5 par d\u00e9faut)</li> </ol>"},{"location":"implementation/faq-system/#extensibilite","title":"Extensibilit\u00e9","text":"<ul> <li>Ajout de questions dans <code>faq.json</code></li> <li>Ajustement du seuil dans <code>settings.ini</code></li> </ul>"},{"location":"implementation/gesture-detection/","title":"D\u00e9tection et Reconnaissance des Gestes","text":""},{"location":"implementation/gesture-detection/#pipeline-de-traitement","title":"Pipeline de Traitement","text":"<ol> <li>Capture vid\u00e9o avec OpenCV</li> <li>D\u00e9tection des mains avec MediaPipe</li> <li>Extraction des landmarks (21 points par main)</li> <li>Classification des gestes</li> <li>Mapping geste \u2192 action</li> </ol>"},{"location":"implementation/gesture-detection/#algorithme-de-reconnaissance","title":"Algorithme de Reconnaissance","text":"<pre><code>def get_gesture(self):\n    # 1. Calcul des distances entre les landmarks\n    dist_index = self.get_dist([8, 5])  # Index\n    dist_middle = self.get_dist([12, 9])  # Majeur\n\n    # 2. Calcul des ratios caract\u00e9ristiques\n    ratio_index = dist_index / dist_middle\n\n    # 3. Classification bas\u00e9e sur les seuils\n    if ratio_index &gt; self.vgest_threshold:\n        return Gest.V_GEST\n    elif self.get_pinch_strength() &gt; self.pinch_threshold:\n        return Gest.PINCH_MAJOR\n    # ...\n</code></pre>"},{"location":"implementation/gesture-detection/#defis-et-solutions","title":"D\u00e9fis et Solutions","text":"<p>Probl\u00e8me : Variabilit\u00e9 des gestes entre utilisateurs Solution : Seuils configurables dans <code>settings.ini</code></p> <p>Probl\u00e8me : Latence du traitement Solution : Thread s\u00e9par\u00e9 pour le traitement vid\u00e9o</p> <p>Probl\u00e8me : Faux positifs Solution : Validation sur plusieurs frames (min_frames_confirm)</p>"},{"location":"implementation/ui-system/","title":"Syst\u00e8me d'Interface Utilisateur","text":""},{"location":"implementation/ui-system/#architecture-pyqt5","title":"Architecture PyQt5","text":"<pre><code>classDiagram\n    class MainWindow {\n        +QTabWidget tab_widget\n        +VideoThread video_thread\n        +init_ui()\n        +update_video_frame()\n    }\n\n    class VideoThread {\n        +QThread\n        +frame_signal\n        +run()\n        +stop()\n    }\n\n    class SettingsDialog {\n        +QDialog\n        +apply_settings()\n    }\n\n    MainWindow --&gt; VideoThread\n    MainWindow --&gt; SettingsDialog</code></pre>"},{"location":"implementation/ui-system/#gestion-du-thread-video","title":"Gestion du Thread Vid\u00e9o","text":"<p>L'interface utilise un pattern Producteur-Consommateur :</p> <pre><code>class VideoThread(QThread):\n    frame_signal = pyqtSignal(QImage, str, str)\n\n    def run(self):\n        while self.running:\n            frame = capture_frame()\n            processed = process_frame(frame)\n            self.frame_signal.emit(processed)\n</code></pre>"},{"location":"implementation/ui-system/#synchronisation-ui-traitement","title":"Synchronisation UI-Traitement","text":"<ol> <li>Le thread vid\u00e9o \u00e9met un signal avec l'image trait\u00e9e</li> <li>Le MainWindow re\u00e7oit le signal et met \u00e0 jour l'UI</li> <li>Pas de blocage de l'interface pendant le traitement</li> </ol>"},{"location":"implementation/ui-system/#personnalisation","title":"Personnalisation","text":"<p>L'interface utilise des styles CSS configurables :</p> <pre><code>[UI]\nstylesheet = \n    QMainWindow { background-color: #FFFFFF; }\n    QPushButton {\n        background-color: #4CAF50; \n        color: white;\n    }\n</code></pre>"}]}